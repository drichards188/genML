# CrewAI Generic ML Pipeline ğŸ¤–

A generalized machine learning pipeline using CrewAI Flows that can work with any dataset following the standard train.csv/test.csv format. Originally designed for the Kaggle Titanic challenge, it has evolved into a flexible ML automation tool.

## Overview

This project demonstrates how to use CrewAI's Flow system to orchestrate a complete machine learning pipeline that automatically adapts to different datasets and problem types:

- **Automated Data Analysis**: Loads and explores any dataset structure
- **Intelligent Feature Engineering**: Uses `AutoFeatureEngine` to automatically detect data types and generate domain-specific features
- **Adaptive Model Training**: Automatically detects problem type (regression vs classification) and trains appropriate models
- **Smart Submission Generation**: Detects submission format from sample files and creates properly formatted outputs

## Project Structure

```
genML/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ CLAUDE.md                    # Development guidance for Claude Code
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ pyproject.toml              # Package configuration
â”œâ”€â”€ run_api.py                  # FastAPI server for dashboard backend
â”œâ”€â”€ src/genML/              # Main source code
â”‚   â”œâ”€â”€ main.py                 # Main entry point
â”‚   â”œâ”€â”€ flow.py                 # CrewAI Flow orchestration
â”‚   â”œâ”€â”€ tools.py                # Backwards-compatible shim (re-exports modern pipeline API)
â”‚   â”œâ”€â”€ submission_formatter.py # Adaptive submission formatting
â”‚   â”œâ”€â”€ api/                    # Dashboard API backend
â”‚   â”œâ”€â”€ pipeline/               # Modular ML pipeline implementation
â”‚   â”‚   â”œâ”€â”€ __init__.py         # Public pipeline API (`load_dataset`, `train_model_pipeline`, ...)
â”‚   â”‚   â”œâ”€â”€ config.py           # Shared constants, paths, and tuning settings
â”‚   â”‚   â”œâ”€â”€ dataset.py          # Dataset discovery / loading utilities
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py # Automated feature engineering + AI ideation integration
â”‚   â”‚   â”œâ”€â”€ model_advisor.py    # Model guidance + problem type detection
â”‚   â”‚   â”œâ”€â”€ training.py         # Optuna tuning, ensemble evaluation, GPU-aware training
â”‚   â”‚   â”œâ”€â”€ prediction.py       # Submission + prediction generation
â”‚   â”‚   â””â”€â”€ utils.py            # Memory / GPU cleanup helpers
â”‚   â””â”€â”€ features/               # Automated feature engineering helpers
â”‚       â”œâ”€â”€ feature_engine.py   # Main feature engineering orchestrator
â”‚       â”œâ”€â”€ data_analyzer.py    # Data type detection
â”‚       â”œâ”€â”€ feature_processors.py # Feature transformation modules
â”‚       â”œâ”€â”€ feature_selector.py # Intelligent feature selection
â”‚       â””â”€â”€ domain_researcher.py # Domain-specific strategies
â”œâ”€â”€ dashboard/                  # Real-time monitoring dashboard
â”‚   â”œâ”€â”€ src/                   # React + TypeScript source
â”‚   â”‚   â”œâ”€â”€ api/               # API client
â”‚   â”‚   â”œâ”€â”€ hooks/             # React hooks (WebSocket, data fetching)
â”‚   â”‚   â”œâ”€â”€ store/             # State management
â”‚   â”‚   â”œâ”€â”€ types/             # TypeScript types
â”‚   â”‚   â””â”€â”€ App.tsx            # Main dashboard component
â”‚   â”œâ”€â”€ package.json           # Node.js dependencies
â”‚   â””â”€â”€ vite.config.ts         # Vite configuration
â”œâ”€â”€ datasets/                   # Organized dataset storage
â”‚   â”œâ”€â”€ current/               # Active dataset (recommended location)
â”‚   â”œâ”€â”€ titanic/               # Titanic-specific data
â”‚   â””â”€â”€ [other]/               # Additional problem domains
â”œâ”€â”€ outputs/                   # Generated artifacts
â”‚   â”œâ”€â”€ data/                  # Processed datasets
â”‚   â”œâ”€â”€ features/              # Feature engineering outputs
â”‚   â”œâ”€â”€ models/                # Trained models
â”‚   â”œâ”€â”€ predictions/           # Prediction files
â”‚   â””â”€â”€ reports/               # Analysis reports
â””â”€â”€ submission.csv             # Main submission file (after running)
```

## Installation

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   # OR using UV (recommended)
   uv sync
   ```

2. **Optional: Install dashboard dependencies**
   ```bash
   # For the real-time monitoring dashboard
   pip install fastapi uvicorn[standard] websockets

   # Frontend dependencies
   cd dashboard
   npm install
   cd ..
   ```

3. **Prepare your dataset:**
   - Place `train.csv` and `test.csv` in `datasets/current/` (recommended)
   - Optionally include `sample_submission.csv` for automatic format detection
   - The pipeline works with any ML dataset following this structure

## Usage

### Quick Start

```bash
# Run the complete ML pipeline
python src/genML/main.py

# Alternative methods
crewai run              # Using CrewAI CLI
uv run kickoff          # Using UV package scripts
```

### What Happens

The pipeline executes in 4 sequential steps using CrewAI Flow orchestration:

1. **Automated Data Analysis** ğŸ“Š
   - Loads train.csv and test.csv from multiple possible locations
   - Analyzes dataset structure, data types, and missing values
   - Provides comprehensive data summary and statistics

2. **Intelligent Feature Engineering** ğŸ”§
   - **AutoFeatureEngine** automatically detects data types (numerical, categorical, text, datetime)
   - Generates domain-specific features based on detected patterns
   - Applies appropriate preprocessing (scaling, encoding, text processing)
   - Performs intelligent feature selection using statistical and model-based methods
   - Adapts to any dataset structure automatically

3. **Adaptive Model Training** ğŸ¤–
   - **Automatic problem type detection** (regression vs classification)
   - Trains appropriate models based on problem type:
     - **Classification**: Logistic Regression, Random Forest, XGBoost Classifier
     - **Regression**: Linear Regression, Random Forest Regressor, XGBoost Regressor
   - Uses proper cross-validation (StratifiedKFold for classification, KFold for regression)
   - Selects best performing model using appropriate metrics

4. **Smart Submission Generation** ğŸ“ˆ
   - **Adaptive submission formatting** detects format from sample_submission.csv
   - Supports binary classification, probability outputs, and regression
   - Generates predictions with confidence scores
   - Creates properly formatted submission files
   - Works with any competition platform format

## Output Files

After successful execution, you'll find:

**Main Files:**
- `submission.csv` - Main submission file ready for upload
- `submission_YYYYMMDD_HHMMSS.csv` - Timestamped backup

**Organized Outputs Directory:**
- `outputs/data/` - Processed datasets (train_data.pkl, test_data.pkl)
- `outputs/features/` - Feature engineering artifacts (X_train.npy, feature_names.txt)
- `outputs/models/` - Trained models (best_model_*.pkl, cross_validation_results.csv)
- `outputs/predictions/` - Detailed predictions and submission files
- `outputs/reports/` - JSON reports from each pipeline stage

## Real-Time Dashboard ğŸ“Š

The project includes a **real-time monitoring dashboard** built with React + TypeScript that provides live updates of pipeline execution.

### Dashboard Features

- **Live Pipeline Status**: Real-time progress tracking via WebSocket
- **Stage Monitoring**: Visual indicators for all 5 pipeline stages (pending/running/completed/failed)
- **Model Training Progress**: Track each model's training status and Optuna trials
- **Resource Monitoring**: GPU memory, CPU, and RAM usage
- **Current Activity**: See exactly what the pipeline is doing right now
- **Debug View**: Raw JSON data explorer for development

### Quick Start - Dashboard

**Prerequisites:**
- Node.js 18+ and npm
- FastAPI dependencies: `pip install fastapi uvicorn[standard] websockets`

**Development Mode (Recommended):**

```bash
# Terminal 1: Start API Backend
python run_api.py

# Terminal 2: Start Dashboard (in new terminal)
cd dashboard
npm install  # First time only
npm run dev

# Terminal 3: Run the ML Pipeline (in new terminal)
python src/genML/main.py
```

Then open your browser to **http://localhost:5173** to watch the pipeline run in real-time!

**Production Mode:**

```bash
# Build and serve the dashboard
cd dashboard
npm run build
cd ..
python run_api.py

# Open browser to http://localhost:8000
```

### Dashboard Architecture

- **Frontend**: React 18 + TypeScript + Vite
- **Backend**: FastAPI with WebSocket support
- **State Management**: Zustand
- **Charts**: Recharts (for future visualizations)
- **API Client**: Axios with React Query

The dashboard auto-connects to the backend and displays real-time updates as the pipeline executes. When no pipeline is running, it shows "No Active Pipeline" status.

**For more details**, see `dashboard/README.md`


## Key Features

### ğŸ¤– Automated Feature Engineering
- **AutoFeatureEngine**: Automatically detects data types and generates relevant features
- **Domain-specific strategies**: Adapts feature generation based on detected problem domains
- **Intelligent feature selection**: Uses statistical tests and model-based selection
- **Configurable processing**: Customizable feature engineering parameters

### ğŸ”„ Adaptive Pipeline
- **Problem type detection**: Automatically identifies regression vs classification
- **Multi-dataset support**: Works with any train.csv/test.csv dataset
- **Smart path resolution**: Finds datasets in multiple organized locations
- **Format detection**: Automatically detects submission format from sample files

### ğŸ—ï¸ CrewAI Flow Architecture
- **Sequential orchestration**: Uses `@start()` and `@listen()` decorators
- **Error handling**: Comprehensive error propagation between pipeline stages
- **Progress tracking**: Real-time status updates and detailed logging
- **Modular design**: Each stage is independent and can be modified separately
- **Pipeline package**: All production logic lives in `src/genML/pipeline/`, while `src/genML/tools.py` keeps the historical API intact for older scripts/tests.

### ğŸ“Š Core ML Functions
The primary entry points are re-exported via `src.genML.pipeline` (and mirrored in `src.genML.tools` for compatibility):
- `load_dataset()` - Multi-path dataset loading with validation
- `engineer_features()` - Automated feature engineering pipeline (with optional AI feature ideation)
- `train_model_pipeline()` - Multi-model training with Optuna tuning, GPU-aware factories, and optional stacking ensembles
- `generate_predictions()` - Adaptive prediction generation and formatting

## Requirements

**Core Dependencies:**
- Python 3.10+ (as specified in pyproject.toml)
- CrewAI 0.193.2+ with tools support
- Standard ML libraries (pandas, scikit-learn, xgboost, numpy)
- Any dataset following train.csv/test.csv format

**Optional - For Real-Time Dashboard:**
- Node.js 18+ and npm
- FastAPI + uvicorn + websockets
- Modern web browser

## Working with Different Datasets

To use the pipeline with a new dataset:

1. **Prepare your data:**
   - Place `train.csv` and `test.csv` in `datasets/current/`
   - Optionally include `sample_submission.csv` for format detection
   - Ensure your train.csv has the target variable in a column

2. **Run the pipeline:**
   ```bash
   conda source genml
   python3 src/genML/main.py
   ```

3. **The pipeline will automatically:**
   - Detect whether it's a regression or classification problem
   - Generate appropriate features based on your data types
   - Train suitable models for your problem type
   - Create properly formatted submission files

## Troubleshooting

**Missing data files:**
```
âŒ Missing required data files: train.csv, test.csv
```
Solution: Place your dataset files in `datasets/current/` or project root.

**Import errors:**
```
ModuleNotFoundError: No module named 'crewai'
```
Solution: `pip install -r requirements.txt` or `uv sync`

**Pipeline fails on feature engineering:**
Check that your dataset has proper column names and no completely empty columns.

**Dashboard not connecting:**
- Ensure FastAPI backend is running: `python run_api.py`
- Check that port 8000 is available
- Verify dashboard is running: `cd dashboard && npm run dev`
- See `dashboard/README.md` for detailed troubleshooting
